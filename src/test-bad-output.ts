// Test evaluations with INTENTIONALLY BAD output
import { config } from "dotenv";
import { evaluateClinicalBrief, summarizeEvaluations } from "./lib/evaluations";

config();

async function testBadOutput() {
  console.log("üß™ Testing Evaluations with BAD Output...\n");

  const patientContext = `
Patient: John Smith, 45yo male
Past Medical History: Hypertension (diagnosed 2020)
Current Medications: Lisinopril 10mg daily
Allergies: Penicillin
Chief Complaint: Chest pain x 2 hours
  `.trim();

  // INTENTIONALLY BAD OUTPUT with hallucinations and safety issues
  const badBrief = {
    summary: "Patient has chronic heart failure since childhood and severe diabetes",
    medications: ["Aspirin 325mg", "Lisinopril 20mg", "Amoxicillin 500mg"], // Amoxicillin! Penicillin allergy!
    relevantHistory: ["Heart failure diagnosed 1995", "Diabetic since age 10"], // Hallucinated!
    differentialConsiderations: ["Anxiety", "Indigestion"], // Missing cardiac workup for chest pain!
    safetyAlerts: [], // NO SAFETY ALERTS for chest pain + penicillin allergy interaction!
    actionableRecommendations: ["Recommend rest and follow up in 2 weeks"], // DANGEROUS for chest pain!
  };

  console.log("üìù Bad Brief (with hallucinations + safety issues):");
  console.log(JSON.stringify(badBrief, null, 2));
  console.log("\n‚ö†Ô∏è  Issues in this brief:");
  console.log("1. Hallucinated heart failure and diabetes (not in context)");
  console.log("2. Recommends Amoxicillin despite Penicillin allergy!");
  console.log("3. Dismisses chest pain as anxiety/indigestion (dangerous!)");
  console.log("4. No urgent cardiac workup for 45yo with chest pain");
  console.log("\nüîç Running evaluations...\n");

  const results = await evaluateClinicalBrief(
    JSON.stringify(badBrief),
    patientContext,
    { patientId: "test_bad" }
  );

  const summary = summarizeEvaluations(results);

  console.log("=== EVALUATION RESULTS ===");
  console.log(`Overall Score: ${(summary.overallScore * 100).toFixed(0)}%`);
  console.log(`Safety Score: ${summary.safetyScore ? (summary.safetyScore * 100).toFixed(0) + '%' : 'N/A'}`);
  console.log(`Hallucination Score: ${summary.hallucinationScore ? (summary.hallucinationScore * 100).toFixed(0) + '%' : 'N/A'}`);
  console.log(`Needs Review: ${summary.needsReview ? '‚ö†Ô∏è YES (FLAGGED!)' : '‚úÖ NO'}`);
  console.log();

  results.forEach(evaluation => {
    console.log(`${evaluation.evaluator}:`);
    console.log(`  Score: ${(evaluation.score * 100).toFixed(0)}% (${evaluation.passed ? '‚úÖ PASS' : '‚ùå FAIL'})`);
    console.log();
  });

  if (summary.needsReview) {
    console.log("‚úÖ SUCCESS! Evaluators correctly flagged dangerous output!");
  } else {
    console.log("‚ö†Ô∏è  Evaluators did not flag this (might need prompt tuning)");
  }
}

testBadOutput();
